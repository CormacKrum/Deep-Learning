{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Your Own Word2Vec Model\n",
    "\n",
    "### *Word2Vec model can learn embeddings from any text corpus!*\n",
    "\n",
    "* Continuous Bag of Words Model\n",
    "* Skip Gram Model\n",
    "\n",
    "Algorithm looks at window of target word(Y) to provide context word(X), the model is trained on (X,Y) pairs in a superwised manner. The algorithm was developed by Tomas Mikolov.\n",
    "\n",
    "### *Data Preparation*\n",
    "\n",
    "* Each sentence must be tokenized, into a list of words.\n",
    "* The sentences can be text loaded into memory once, or we can build a data pipeline which iteratively feeds data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords  = set(stopwords.words('english'))\n",
    "def readFile(file):\n",
    "    f = open(file,'r',encoding='utf-8')\n",
    "    text = f.read()\n",
    "    \n",
    "    data = []\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent) \n",
    "        words = [w.lower() for w in words if len(w)>2 and w not in stopWords]\n",
    "        data.append(words)\n",
    "    \n",
    "    return data\n",
    "\n",
    "text = readFile(\"bollywood.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=116, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(text,size=300,window=10,min_count=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepika', 'padukone', 'ranveer', 'singh', 'wedding', 'one', 'biggest', 'bollywood', 'events', 'happened', '2018', 'the', 'celebrations', 'hooked', 'phones', 'waiting', 'come', 'also', 'gave', 'enough', 'reason', 'believe', 'stylish', 'two', 'couple', 'from', 'airport', 'looks', 'reception', 'parties', 'everything', 'entire', 'timeline', 'style', 'file', 'not', 'ambanis', 'priyanka', 'nick', 'man', 'proves', 'year', 'this', 'big', 'fat', 'lavish', 'extravagant', 'weddings', 'isha', 'ambani', 'anand', 'piramal', 'chopra', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', 'saw', 'many', 'grand', 'but', 'nothing', 'beats', 'award', 'social', 'media', 'shared', 'video', 'featuring', 'jonaswas', 'celebrating', 'family', 'first', 'celebrated', 'christmas', 'london', 'pictures', 'new', 'outstanding', 'glimpses', 'celebration', 'verbier', 'switzerland', 'married', 'december', 'three', 'receptions', 'delhi', 'mumbai', 'jaggo', 'night', 'made', 'even', 'special', 'industry', 'friends', 'long', 'time', 'there', 'virat', 'side', 'actress', 'wife', 'anushka', 'pleasure', 'audience', 'while', 'rang', 'morning', 'dress', 'squad', 'attire', 'pink', 'salwar', 'suit']\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_actor(a,b,c,word_vectors):\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    max_similarity = -100 \n",
    "    d = None\n",
    "    words = [\"ranveer\",\"deepika\",\"padukone\",\"singh\",\"nick\",\"jonas\",\"chopra\",\"priyanka\",\"virat\",\"anushka\",\"ginni\"]\n",
    "\n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    for w in words:\n",
    "        \n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "        \n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        \n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chopra\n"
     ]
    }
   ],
   "source": [
    "print(predict_actor(\"deepika\",\"ranveer\",\"priyanka\",model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
